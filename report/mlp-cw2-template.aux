\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{icml2017}
\citation{simonyan2014very,he2016deep}
\citation{glorot2010understanding}
\citation{bengio1993problem}
\citation{he2016deep}
\citation{glorot2010understanding}
\citation{glorot2010understanding}
\citation{bishop1995neural}
\citation{ioffe2015batch}
\citation{he2016deep,huang2017densely}
\citation{ioffe2015batch}
\citation{he2016deep}
\citation{rumelhart1986learning}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:loss_curves}{{1a}{1}{Loss per epoch\relax }{figure.caption.1}{}}
\newlabel{sub@fig:loss_curves}{{a}{1}{Loss per epoch\relax }{figure.caption.1}{}}
\newlabel{fig:acc_curves}{{1b}{1}{Accuracy per epoch\relax }{figure.caption.1}{}}
\newlabel{sub@fig:acc_curves}{{b}{1}{Accuracy per epoch\relax }{figure.caption.1}{}}
\newlabel{fig:curves}{{1}{1}{Training curves for VGG08 and VGG38\relax }{figure.caption.1}{}}
\newlabel{sec:task1}{{2}{1}{}{section.2}{}}
\citation{bengio1993problem}
\citation{simonyan2014very}
\citation{ioffe2015batch}
\citation{lecun2012efficient}
\citation{santurkar2018does}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\newlabel{fig:grad_flow_08}{{2}{2}{Gradient flow on VGG08\relax }{figure.caption.2}{}}
\newlabel{fig:grad_flow_38}{{3}{2}{Gradient Flow on VGG38\relax }{figure.caption.3}{}}
\newlabel{eq.fprop}{{1}{2}{}{equation.2.1}{}}
\newlabel{eq.bprop}{{2}{2}{}{equation.2.2}{}}
\newlabel{sec:lit_rev}{{3}{2}{}{section.3}{}}
\citation{he2016deep}
\citation{he2016deep}
\citation{krizhevsky2009learning}
\newlabel{eq.grad_skip}{{3}{3}{}{equation.4.3}{}}
\newlabel{fig:grad_flow_bestModel}{{4}{3}{Training curves for ? ? ?\relax }{figure.caption.4}{}}
\newlabel{fig:grad_flow_bestModel}{{5}{3}{Gradient Flow on ? ? ?\relax }{figure.caption.5}{}}
\citation{he2016deep}
\citation{huang2017densely}
\citation{bengio1993problem}
\newlabel{tab:CIFAR_results}{{1}{4}{Experiment results (number of model parameters, Training and Validation loss and accuracy) for different combinations of VGG08, VGG38, Batch Normalisation (BN), and Residual Connections (RC), LR is learning rate.\relax }{table.caption.6}{}}
\newlabel{subsec:rescimp}{{5.1}{4}{}{subsection.5.1}{}}
\newlabel{sec:disc}{{6}{4}{}{section.6}{}}
\newlabel{sec:concl}{{7}{4}{}{section.7}{}}
\bibdata{refs}
\gdef \@abspage@last{5}
