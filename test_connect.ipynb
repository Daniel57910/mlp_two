{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import mlp.data_providers as data_providers\n",
    "from pytorch_mlp_framework.model_architectures import *\n",
    "from pytorch_mlp_framework.model_architectures_dm import ( ConvolutionalProcessingBlockDM, ConvolutionalReductionBlockDM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP2022_23_CW2_Spec.pdf       mlp-cw2-template.log\n",
      "README.md\t\t      notebooks\n",
      "V88_08_experiment\t      notes\n",
      "VGG_08\t\t\t      pytest.ini\n",
      "VGG_08_experiment\t      pytorch_mlp_framework\n",
      "VGG_38\t\t\t      report\n",
      "VGG_38_experiment\t      report.zip\n",
      "VGG_38_experiment_batch_norm  requirements.txt\n",
      "__init__.py\t\t      run_vgg_08_default.sh\n",
      "data\t\t\t      run_vgg_38_default.sh\n",
      "gradP.png\t\t      scripts\n",
      "grad_flow.png\t\t      setup.py\n",
      "grad_flow_38.pdf\t      test_connect.ipynb\n",
      "install.sh\t\t      test_network\n",
      "mlp\t\t\t      vgg_38.json\n",
      "mlp-cw2-template.aux\t      vgg_38_gradients.json\n",
      "mlp-cw2-template.dvi\t      vgg_38_out.pdf\n",
      "mlp-cw2-template.fdb_latexmk  vgg_38_out.png\n",
      "mlp-cw2-template.fls\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!touch __init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    batch_size = 100\n",
    "    continue_from_epoch = -1\n",
    "    seed = 0\n",
    "    image_num_channels = 3\n",
    "    image_height = 32\n",
    "    image_width = 32\n",
    "    num_stages = 3\n",
    "    num_blocks_per_stage = 8\n",
    "    num_filters = 32\n",
    "    num_epochs = 1\n",
    "    num_classes = 100\n",
    "    experiment_name = 'VGG_08_experiment'\n",
    "    use_gpu = True\n",
    "    weight_decay_coefficient = 0\n",
    "    block_type = 'conv_block'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pytorch_mlp_framework.storage_utils import save_statistics\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 8})\n",
    "\n",
    "class ExperimentBuilder(nn.Module):\n",
    "    def __init__(self, network_model, experiment_name, num_epochs, train_data, val_data,\n",
    "                 test_data, weight_decay_coefficient, use_gpu, continue_from_epoch=-1):\n",
    "        \"\"\"\n",
    "        Initializes an ExperimentBuilder object. Such an object takes care of running training and evaluation of a deep net\n",
    "        on a given dataset. It also takes care of saving per epoch models and automatically inferring the best val model\n",
    "        to be used for evaluating the test set metrics.\n",
    "        :param network_model: A pytorch nn.Module which implements a network architecture.\n",
    "        :param experiment_name: The name of the experiment. This is used mainly for keeping track of the experiment and creating and directory structure that will be used to save logs, model parameters and other.\n",
    "        :param num_epochs: Total number of epochs to run the experiment\n",
    "        :param train_data: An object of the DataProvider type. Contains the training set.\n",
    "        :param val_data: An object of the DataProvider type. Contains the val set.\n",
    "        :param test_data: An object of the DataProvider type. Contains the test set.\n",
    "        :param weight_decay_coefficient: A float indicating the weight decay to use with the adam optimizer.\n",
    "        :param use_gpu: A boolean indicating whether to use a GPU or not.\n",
    "        :param continue_from_epoch: An int indicating whether we'll start from scrach (-1) or whether we'll reload a previously saved model of epoch 'continue_from_epoch' and continue training from there.\n",
    "        \"\"\"\n",
    "        super(ExperimentBuilder, self).__init__()\n",
    "\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model = network_model\n",
    "\n",
    "        if torch.cuda.device_count() > 1 and use_gpu:\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model.to(self.device)\n",
    "            self.model = nn.DataParallel(module=self.model)\n",
    "            print('Use Multi GPU', self.device)\n",
    "        elif torch.cuda.device_count() == 1 and use_gpu:\n",
    "            self.device =  torch.cuda.current_device()\n",
    "            self.model.to(self.device)  # sends the model from the cpu to the gpu\n",
    "            print('Use GPU', self.device)\n",
    "        else:\n",
    "            print(\"use CPU\")\n",
    "            self.device = torch.device('cpu')  # sets the device to be CPU\n",
    "            print(self.device)\n",
    "\n",
    "        print('here')\n",
    "\n",
    "        self.model.reset_parameters()  # re-initialize network parameters\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.all_gradients = []\n",
    "\n",
    "        print('System learnable parameters')\n",
    "        num_conv_layers = 0\n",
    "        num_linear_layers = 0\n",
    "        total_num_parameters = 0\n",
    "        for name, value in self.named_parameters():\n",
    "            print(name, value.shape)\n",
    "            if all(item in name for item in ['conv', 'weight']):\n",
    "                num_conv_layers += 1\n",
    "            if all(item in name for item in ['linear', 'weight']):\n",
    "                num_linear_layers += 1\n",
    "            total_num_parameters += np.prod(value.shape)\n",
    "\n",
    "        print('Total number of parameters', total_num_parameters)\n",
    "        print('Total number of conv layers', num_conv_layers)\n",
    "        print('Total number of linear layers', num_linear_layers)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), amsgrad=False,\n",
    "                                    weight_decay=weight_decay_coefficient)\n",
    "        self.learning_rate_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer,\n",
    "                                                                            T_max=num_epochs,\n",
    "                                                                            eta_min=0.00002)\n",
    "        # Generate the directory names\n",
    "        self.experiment_folder = os.path.abspath(experiment_name)\n",
    "        self.experiment_logs = os.path.abspath(os.path.join(self.experiment_folder, \"result_outputs\"))\n",
    "        self.experiment_saved_models = os.path.abspath(os.path.join(self.experiment_folder, \"saved_models\"))\n",
    "\n",
    "        # Set best models to be at 0 since we are just starting\n",
    "        self.best_val_model_idx = 0\n",
    "        self.best_val_model_acc = 0.\n",
    "\n",
    "        if not os.path.exists(self.experiment_folder):  # If experiment directory does not exist\n",
    "            os.mkdir(self.experiment_folder)  # create the experiment directory\n",
    "            os.mkdir(self.experiment_logs)  # create the experiment log directory\n",
    "            os.mkdir(self.experiment_saved_models)  # create the experiment saved models directory\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)  # send the loss computation to the GPU\n",
    "\n",
    "        if continue_from_epoch == -2:  # if continue from epoch is -2 then continue from latest saved model\n",
    "            self.state, self.best_val_model_idx, self.best_val_model_acc = self.load_model(\n",
    "                model_save_dir=self.experiment_saved_models, model_save_name=\"train_model\",\n",
    "                model_idx='latest')  # reload existing model from epoch and return best val model index\n",
    "            # and the best val acc of that model\n",
    "            self.starting_epoch = int(self.state['model_epoch'])\n",
    "\n",
    "        elif continue_from_epoch > -1:  # if continue from epoch is greater than -1 then\n",
    "            self.state, self.best_val_model_idx, self.best_val_model_acc = self.load_model(\n",
    "                model_save_dir=self.experiment_saved_models, model_save_name=\"train_model\",\n",
    "                model_idx=continue_from_epoch)  # reload existing model from epoch and return best val model index\n",
    "            # and the best val acc of that model\n",
    "            self.starting_epoch = continue_from_epoch\n",
    "        else:\n",
    "            self.state = dict()\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def get_num_parameters(self):\n",
    "        total_num_params = 0\n",
    "        for param in self.parameters():\n",
    "            total_num_params += np.prod(param.shape)\n",
    "\n",
    "        return total_num_params\n",
    "\n",
    "\n",
    "    def plot_func_def(self,all_grads, layers):\n",
    "        \n",
    "       \n",
    "        \"\"\"\n",
    "        Plot function definition to plot the average gradient with respect to the number of layers in the given model\n",
    "        :param all_grads: Gradients wrt weights for each layer in the model.\n",
    "        :param layers: Layer names corresponding to the model parameters\n",
    "        :return: plot for gradient flow\n",
    "        \"\"\"\n",
    "        plt.plot(all_grads, alpha=0.3, color=\"b\")\n",
    "        plt.hlines(0, 0, len(all_grads)+1, linewidth=1, color=\"k\" )\n",
    "        plt.xticks(range(0,len(all_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(xmin=0, xmax=len(all_grads))\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"Average Gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt\n",
    "        \n",
    "    \n",
    "    def plot_grad_flow(self, named_parameters):\n",
    "        \"\"\"\n",
    "        The function is being called in Line 298 of this file. \n",
    "        Receives the parameters of the model being trained. Returns plot of gradient flow for the given model parameters.\n",
    "       \n",
    "        \"\"\"\n",
    "        all_grads = []\n",
    "        layers = []\n",
    "\n",
    "        for name, p in named_parameters:\n",
    "            if (p.requires_grad) and (\"bias\" not in name):\n",
    "                name = name.replace(\"layer_dict.\", \"\")\n",
    "                name = name.replace(\".weight\", \"\")\n",
    "                layers.append(name)\n",
    "                all_grads.append(p.grad.abs().mean().cpu().numpy())\n",
    "        \"\"\"\n",
    "        Complete the code in the block below to collect absolute mean of the gradients for each layer in all_grads with the             layer names in layers.\n",
    "        \"\"\"\n",
    "        ########################################\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "            \n",
    "        \n",
    "        plt = self.plot_func_def(all_grads, layers)\n",
    "        \n",
    "        return plt, all_grads, layers\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_train_iter(self, x, y):\n",
    "        \n",
    "        self.train()  # sets model to training mode (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "        x, y = x.float().to(device=self.device), y.long().to(\n",
    "            device=self.device)  # send data to device as torch tensors\n",
    "        out = self.model.forward(x)  # forward the data in the model\n",
    "\n",
    "\n",
    "        loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "\n",
    "        self.optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n",
    "        loss.backward()  # backpropagate to compute gradients for current iter loss\n",
    "        \n",
    "        self.learning_rate_scheduler.step(epoch=self.current_epoch)\n",
    "        self.optimizer.step()  # update network parameters\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "        return loss.cpu().data.numpy(), accuracy\n",
    "\n",
    "    def run_evaluation_iter(self, x, y):\n",
    "        \"\"\"\n",
    "        Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "        :return: the loss and accuracy for this batch\n",
    "        \"\"\"\n",
    "        self.eval()  # sets the system to validation mode\n",
    "        x, y = x.float().to(device=self.device), y.long().to(\n",
    "            device=self.device)  # convert data to pytorch tensors and send to the computation device\n",
    "        out = self.model.forward(x)  # forward the data in the model\n",
    "\n",
    "        loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "        return loss.cpu().data.numpy(), accuracy\n",
    "\n",
    "    def save_model(self, model_save_dir, model_save_name, model_idx, best_validation_model_idx,\n",
    "                   best_validation_model_acc):\n",
    "        \"\"\"\n",
    "        Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "        :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param state: The dictionary containing the system state.\n",
    "\n",
    "        \"\"\"\n",
    "        self.state['network'] = self.state_dict()  # save network parameter and other variables.\n",
    "        self.state['best_val_model_idx'] = best_validation_model_idx  # save current best val idx\n",
    "        self.state['best_val_model_acc'] = best_validation_model_acc  # save current best val acc\n",
    "        torch.save(self.state, f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(\n",
    "            model_idx))))  # save state at prespecified filepath\n",
    "\n",
    "    def load_model(self, model_save_dir, model_save_name, model_idx):\n",
    "        \"\"\"\n",
    "        Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :return: best val idx and best val model acc, also it loads the network state into the system state without returning it\n",
    "        \"\"\"\n",
    "        state = torch.load(f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx))))\n",
    "        self.load_state_dict(state_dict=state['network'])\n",
    "        return state, state['best_val_model_idx'], state['best_val_model_acc']\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"\n",
    "        Runs experiment train and evaluation iterations, saving the model and best val model and val model accuracy after each epoch\n",
    "        :return: The summary current_epoch_losses from starting epoch to total_epochs.\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        total_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [],\n",
    "                        \"val_loss\": []}  # initialize a dict to keep the per-epoch metrics\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # get current time\n",
    "        train_summary_file = f'{self.experiment_name}_{current_time}_train_summary.csv'  # create a filename for the train summary\n",
    "        val_summary_file = f'{self.experiment_name}_{current_time}_val_summary.csv'  # create a filename for the val summary\n",
    "        for i, epoch_idx in enumerate(range(self.starting_epoch, self.num_epochs)):\n",
    "            epoch_start_time = time.time()\n",
    "            current_epoch_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
    "            self.current_epoch = epoch_idx\n",
    "            with tqdm.tqdm(total=len(self.train_data)) as pbar_train:  # create a progress bar for training\n",
    "                for idx, (x, y) in enumerate(self.train_data):  # get data batches\n",
    "                    loss, accuracy = self.run_train_iter(x=x, y=y)  # take a training iter step\n",
    "                    current_epoch_losses[\"train_loss\"].append(loss)  # add current iter loss to the train loss list\n",
    "                    current_epoch_losses[\"train_acc\"].append(accuracy)  # add current iter acc to the train acc list\n",
    "                    pbar_train.update(1)\n",
    "                    pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "            with tqdm.tqdm(total=len(self.val_data)) as pbar_val:  # create a progress bar for validation\n",
    "                for x, y in self.val_data:  # get data batches\n",
    "                    loss, accuracy = self.run_evaluation_iter(x=x, y=y)  # run a validation iter\n",
    "                    current_epoch_losses[\"val_loss\"].append(loss)  # add current iter loss to val loss list.\n",
    "                    current_epoch_losses[\"val_acc\"].append(accuracy)  # add current iter acc to val acc lst.\n",
    "                    pbar_val.update(1)  # add 1 step to the progress bar\n",
    "                    pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "            val_mean_accuracy = np.mean(current_epoch_losses['val_acc'])\n",
    "            if val_mean_accuracy > self.best_val_model_acc:  # if current epoch's mean val acc is greater than the saved best val acc then\n",
    "                self.best_val_model_acc = val_mean_accuracy  # set the best val model acc to be current epoch's val accuracy\n",
    "                self.best_val_model_idx = epoch_idx  # set the experiment-wise best val idx to be the current epoch's idx\n",
    "\n",
    "            for key, value in current_epoch_losses.items():\n",
    "                total_losses[key].append(np.mean(\n",
    "                    value))  # get mean of all metrics of current epoch metrics dict, to get them ready for storage and output on the terminal.\n",
    "\n",
    "            save_statistics(experiment_log_dir=self.experiment_logs, filename=train_summary_file,\n",
    "                            stats_dict=total_losses, current_epoch=i,\n",
    "                            continue_from_mode=True if (self.starting_epoch != 0 or i > 0) else False)  # save statistics to stats file.\n",
    "\n",
    "            # load_statistics(experiment_log_dir=self.experiment_logs, filename='summary.csv') # How to load a csv file if you need to\n",
    "\n",
    "            out_string = \"_\".join(\n",
    "                [\"{}_{:.4f}\".format(key, np.mean(value)) for key, value in current_epoch_losses.items()])\n",
    "            # create a string to use to report our epoch metrics\n",
    "            epoch_elapsed_time = time.time() - epoch_start_time  # calculate time taken for epoch\n",
    "            epoch_elapsed_time = \"{:.4f}\".format(epoch_elapsed_time)\n",
    "            print(\"Epoch {}:\".format(epoch_idx), out_string, \"epoch time\", epoch_elapsed_time, \"seconds\")\n",
    "            self.state['model_epoch'] = epoch_idx\n",
    "            self.save_model(model_save_dir=self.experiment_saved_models,\n",
    "                            # save model and best val idx and best val acc, using the model dir, model name and model idx\n",
    "                            model_save_name=\"train_model\", model_idx=epoch_idx,\n",
    "                            best_validation_model_idx=self.best_val_model_idx,\n",
    "                            best_validation_model_acc=self.best_val_model_acc)\n",
    "            self.save_model(model_save_dir=self.experiment_saved_models,\n",
    "                            # save model and best val idx and best val acc, using the model dir, model name and model idx\n",
    "                            model_save_name=\"train_model\", model_idx='latest',\n",
    "                            best_validation_model_idx=self.best_val_model_idx,\n",
    "                            best_validation_model_acc=self.best_val_model_acc)\n",
    "            \n",
    "            ################################################################\n",
    "            ##### Plot Gradient Flow at each Epoch during Training  ######\n",
    "            print(\"Generating Gradient Flow Plot at epoch {}\".format(epoch_idx))\n",
    "            gradient_plt, gradients, layers = self.plot_grad_flow(self.model.named_parameters())\n",
    "            self.all_gradients.append(\n",
    "                {\n",
    "                    \"epoch\": epoch_idx,\n",
    "                    \"gradients\": gradients,\n",
    "                    \"layers\": layers\n",
    "                }\n",
    "            )\n",
    "            if not os.path.exists(os.path.join(self.experiment_saved_models, 'gradient_flow_plots')):\n",
    "                os.mkdir(os.path.join(self.experiment_saved_models, 'gradient_flow_plots'))\n",
    "                plt.legend(loc=\"best\")\n",
    "\n",
    "            print('Saving Gradient Flow Plot at epoch {}'.format(epoch_idx))\n",
    "            gradient_plt.savefig(\n",
    "                os.path.join(self.experiment_saved_models, 'gradient_flow_plots', \"epoch{}.png\".format(str(epoch_idx))), \n",
    "                dpi=600\n",
    "            )\n",
    "            ################################################################\n",
    "        \n",
    "        print(\"Generating test set evaluation metrics\")\n",
    "        self.load_model(model_save_dir=self.experiment_saved_models, model_idx=self.best_val_model_idx,\n",
    "                        # load best validation model\n",
    "                        model_save_name=\"train_model\")\n",
    "        current_epoch_losses = {\"test_acc\": [], \"test_loss\": []}  # initialize a statistics dict\n",
    "        with tqdm.tqdm(total=len(self.test_data)) as pbar_test:  # ini a progress bar\n",
    "            for x, y in self.test_data:  # sample batch\n",
    "                loss, accuracy = self.run_evaluation_iter(x=x,\n",
    "                                                          y=y)  # compute loss and accuracy by running an evaluation step\n",
    "                current_epoch_losses[\"test_loss\"].append(loss)  # save test loss\n",
    "                current_epoch_losses[\"test_acc\"].append(accuracy)  # save test accuracy\n",
    "                pbar_test.update(1)  # update progress bar status\n",
    "                pbar_test.set_description(\n",
    "                    \"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))  # update progress bar string output\n",
    "        test_losses = {key: [np.mean(value)] for key, value in\n",
    "                       current_epoch_losses.items()}  # save test set metrics in dict format\n",
    "        save_statistics(experiment_log_dir=self.experiment_logs, filename=val_summary_file,\n",
    "                        # save test set metrics on disk in .csv format\n",
    "                        stats_dict=test_losses, current_epoch=0, continue_from_mode=False)\n",
    "\n",
    "        return total_losses, test_losses, gradient_plt, self.all_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_classification_system(args):\n",
    "\n",
    "    # print all arguments with their values\n",
    "    rng = np.random.RandomState(seed=args.seed)  # set the seeds for the experiment\n",
    "    torch.manual_seed(seed=args.seed)  # sets pytorch's seed\n",
    "\n",
    "# # set up data augmentation transforms for training and testing\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    train_data = data_providers.CIFAR100(root='data', set_name='train',\n",
    "                 transform=transform_train,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "    val_data = data_providers.CIFAR100(root='data', set_name='val',\n",
    "                 transform=transform_test,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "    test_data = data_providers.CIFAR100(root='data', set_name='test',\n",
    "                 transform=transform_test,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "\n",
    "    train_data_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if args.block_type == 'conv_block':\n",
    "        processing_block_type = ConvolutionalProcessingBlock\n",
    "        dim_reduction_block_type = ConvolutionalDimensionalityReductionBlock\n",
    "    elif args.block_type == 'batchnorm_block':\n",
    "        processing_block_type = ConvolutionalProcessingBlockDM\n",
    "        dim_reduction_block_type = ConvolutionalReductionBlockDM\n",
    "    elif args.block_type == 'empty_block':\n",
    "        processing_block_type = EmptyBlock\n",
    "        dim_reduction_block_type = EmptyBlock\n",
    "    else:\n",
    "        raise ModuleNotFoundError\n",
    "\n",
    "    custom_conv_net = ConvolutionalNetwork(\n",
    "        input_shape=(\n",
    "            args.batch_size, args.image_num_channels, args.image_height, args.image_width),\n",
    "        num_output_classes=args.num_classes, num_filters=args.num_filters, use_bias=False,\n",
    "        num_blocks_per_stage=args.num_blocks_per_stage, num_stages=args.num_stages,\n",
    "        processing_block_type=processing_block_type,\n",
    "        dimensionality_reduction_block_type=dim_reduction_block_type)\n",
    "\n",
    "    print('Setup conv net')\n",
    "\n",
    "    conv_experiment = ExperimentBuilder(\n",
    "        network_model=custom_conv_net,\n",
    "        experiment_name=args.experiment_name,\n",
    "        num_epochs=args.num_epochs,\n",
    "        weight_decay_coefficient=args.weight_decay_coefficient,\n",
    "        use_gpu=args.use_gpu,\n",
    "        continue_from_epoch=args.continue_from_epoch,\n",
    "        train_data=train_data_loader, val_data=val_data_loader,\n",
    "        test_data=test_data_loader\n",
    "    )\n",
    "\n",
    "    print('Experiment setup')\n",
    "    return conv_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for vgg 38\n",
    "\n",
    "def vgg_38_args():\n",
    "    args = Arguments\n",
    "    args.num_epochs= 100\n",
    "    args.num_stages = 3\n",
    "    args.num_blocks_per_stage = 5\n",
    "    args.batch_size = 100\n",
    "    args.experiment_name = \"VGG_38_experiment\"\n",
    "    return args\n",
    "\n",
    "def vgg_08_args():\n",
    "    args = Arguments\n",
    "    args.num_epochs= 2\n",
    "    args.num_stages = 3\n",
    "    args.num_blocks_per_stage = 0\n",
    "    args.batch_size = 100\n",
    "    args.experiment_name = \"VGG_08_experiment\"\n",
    "    return args\n",
    "\n",
    "def vgg_38_batch_norm():\n",
    "    args = vgg_38_args()\n",
    "    args.experiment_name = \"VGG_38_experiment_batch_norm\"\n",
    "    args.block_type = 'batchnorm_block'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train (47500, 32, 32, 3)\n",
      "train (47500,)\n",
      "Files already downloaded and verified\n",
      "val (2500, 32, 32, 3)\n",
      "val (2500,)\n",
      "Files already downloaded and verified\n",
      "test (10000, 32, 32, 3)\n",
      "test (10000,)\n",
      "Building basic block of ConvolutionalNetwork using input shape (100, 3, 32, 32)\n",
      "torch.Size([100, 32, 32, 32])\n",
      "shape before final linear layer torch.Size([100, 32, 1, 1])\n",
      "Block is built, output volume is torch.Size([100, 100])\n",
      "Setup conv net\n",
      "Use GPU 0\n",
      "here\n",
      "System learnable parameters\n",
      "model.layer_dict.input_conv.layer_dict.conv_0.weight torch.Size([32, 3, 3, 3])\n",
      "model.layer_dict.input_conv.layer_dict.bn_0.weight torch.Size([32])\n",
      "model.layer_dict.input_conv.layer_dict.bn_0.bias torch.Size([32])\n",
      "model.layer_dict.block_0_0.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_0.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_0_0.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_0_0.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_0.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_0_0.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_0_1.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_1.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_0_1.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_0_1.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_1.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_0_1.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_0_2.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_2.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_0_2.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_0_2.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_2.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_0_2.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_0_3.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_3.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_0_3.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_0_3.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_3.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_0_3.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_0_4.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_4.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_0_4.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_0_4.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_0_4.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_0_4.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_0.layer_dict.conv1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_0.layer_dict.conv2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_0.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_1_0.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_0.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_1_0.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_1_0.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_0.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_1_0.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_1_1.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_1.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_1_1.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_1_1.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_1.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_1_1.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_1_2.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_2.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_1_2.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_1_2.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_2.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_1_2.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_1_3.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_3.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_1_3.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_1_3.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_3.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_1_3.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_1_4.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_4.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_1_4.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_1_4.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_1_4.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_1_4.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_1.layer_dict.conv1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_1.layer_dict.conv2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_1.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_2_0.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_0.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_2_0.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_2_0.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_0.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_2_0.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_2_1.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_1.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_2_1.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_2_1.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_1.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_2_1.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_2_2.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_2.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_2_2.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_2_2.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_2.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_2_2.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_2_3.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_3.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_2_3.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_2_3.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_3.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_2_3.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.block_2_4.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_4.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.block_2_4.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.block_2_4.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.block_2_4.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.block_2_4.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_2.layer_dict.conv1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.batch_norm1.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.batch_norm1.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "model.layer_dict.reduction_block_2.layer_dict.conv2.bias torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.batch_norm2.weight torch.Size([32])\n",
      "model.layer_dict.reduction_block_2.layer_dict.batch_norm2.bias torch.Size([32])\n",
      "model.logit_linear_layer.weight torch.Size([100, 32])\n",
      "model.logit_linear_layer.bias torch.Size([100])\n",
      "Total number of parameters 338500\n",
      "Total number of conv layers 38\n",
      "Total number of linear layers 1\n",
      "Experiment setup\n"
     ]
    }
   ],
   "source": [
    "args = vgg_38_batch_norm()\n",
    "args.__dict__\n",
    "conv_experiment = train_eval_classification_system(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.1216, accuracy: 0.4500: 100%|██████████| 475/475 [00:12<00:00, 39.14it/s]\n",
      "loss: 2.2492, accuracy: 0.4200: 100%|██████████| 25/25 [00:00<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_acc_0.4572_train_loss_1.9793_val_acc_0.4100_val_loss_2.2606 epoch time 12.7366 seconds\n",
      "Generating Gradient Flow Plot at epoch 0\n",
      "Saving Gradient Flow Plot at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.0505, accuracy: 0.4400: 100%|██████████| 475/475 [00:12<00:00, 38.81it/s]\n",
      "loss: 2.0218, accuracy: 0.4200: 100%|██████████| 25/25 [00:00<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc_0.4567_train_loss_1.9733_val_acc_0.3944_val_loss_2.3250 epoch time 12.8725 seconds\n",
      "Generating Gradient Flow Plot at epoch 1\n",
      "Saving Gradient Flow Plot at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.6863, accuracy: 0.5400: 100%|██████████| 475/475 [00:12<00:00, 38.53it/s]\n",
      "loss: 2.9491, accuracy: 0.2900: 100%|██████████| 25/25 [00:00<00:00, 38.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_acc_0.4636_train_loss_1.9574_val_acc_0.3808_val_loss_2.4034 epoch time 12.9830 seconds\n",
      "Generating Gradient Flow Plot at epoch 2\n",
      "Saving Gradient Flow Plot at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.1184, accuracy: 0.5100: 100%|██████████| 475/475 [00:12<00:00, 38.16it/s]\n",
      "loss: 2.2553, accuracy: 0.4300: 100%|██████████| 25/25 [00:00<00:00, 41.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_acc_0.4636_train_loss_1.9493_val_acc_0.4168_val_loss_2.1811 epoch time 13.0594 seconds\n",
      "Generating Gradient Flow Plot at epoch 3\n",
      "Saving Gradient Flow Plot at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7001, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 38.95it/s]\n",
      "loss: 1.9909, accuracy: 0.5000: 100%|██████████| 25/25 [00:00<00:00, 36.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_acc_0.4678_train_loss_1.9336_val_acc_0.4044_val_loss_2.2813 epoch time 12.8775 seconds\n",
      "Generating Gradient Flow Plot at epoch 4\n",
      "Saving Gradient Flow Plot at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8677, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 39.16it/s]\n",
      "loss: 2.2107, accuracy: 0.3800: 100%|██████████| 25/25 [00:00<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_acc_0.4708_train_loss_1.9310_val_acc_0.4160_val_loss_2.1806 epoch time 12.7277 seconds\n",
      "Generating Gradient Flow Plot at epoch 5\n",
      "Saving Gradient Flow Plot at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9170, accuracy: 0.4400: 100%|██████████| 475/475 [00:12<00:00, 38.83it/s]\n",
      "loss: 2.1718, accuracy: 0.4200: 100%|██████████| 25/25 [00:00<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_acc_0.4756_train_loss_1.9081_val_acc_0.4244_val_loss_2.2126 epoch time 12.8618 seconds\n",
      "Generating Gradient Flow Plot at epoch 6\n",
      "Saving Gradient Flow Plot at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9419, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 39.12it/s]\n",
      "loss: 1.9289, accuracy: 0.4800: 100%|██████████| 25/25 [00:00<00:00, 41.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_acc_0.4744_train_loss_1.9033_val_acc_0.4336_val_loss_2.1711 epoch time 12.7597 seconds\n",
      "Generating Gradient Flow Plot at epoch 7\n",
      "Saving Gradient Flow Plot at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7486, accuracy: 0.5500: 100%|██████████| 475/475 [00:12<00:00, 38.60it/s]\n",
      "loss: 2.1961, accuracy: 0.4900: 100%|██████████| 25/25 [00:00<00:00, 38.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_acc_0.4792_train_loss_1.8922_val_acc_0.4300_val_loss_2.1210 epoch time 12.9575 seconds\n",
      "Generating Gradient Flow Plot at epoch 8\n",
      "Saving Gradient Flow Plot at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9135, accuracy: 0.4300: 100%|██████████| 475/475 [00:11<00:00, 39.67it/s]\n",
      "loss: 2.2360, accuracy: 0.4900: 100%|██████████| 25/25 [00:00<00:00, 41.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_acc_0.4822_train_loss_1.8731_val_acc_0.4176_val_loss_2.2458 epoch time 12.7924 seconds\n",
      "Generating Gradient Flow Plot at epoch 9\n",
      "Saving Gradient Flow Plot at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9593, accuracy: 0.4700: 100%|██████████| 475/475 [00:12<00:00, 38.89it/s]\n",
      "loss: 1.7173, accuracy: 0.5200: 100%|██████████| 25/25 [00:00<00:00, 41.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_acc_0.4856_train_loss_1.8614_val_acc_0.4264_val_loss_2.1784 epoch time 12.8272 seconds\n",
      "Generating Gradient Flow Plot at epoch 10\n",
      "Saving Gradient Flow Plot at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7093, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 38.96it/s]\n",
      "loss: 2.0981, accuracy: 0.4600: 100%|██████████| 25/25 [00:00<00:00, 41.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_acc_0.4860_train_loss_1.8577_val_acc_0.4316_val_loss_2.1247 epoch time 12.8030 seconds\n",
      "Generating Gradient Flow Plot at epoch 11\n",
      "Saving Gradient Flow Plot at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.6175, accuracy: 0.5700: 100%|██████████| 475/475 [00:12<00:00, 38.27it/s]\n",
      "loss: 2.0001, accuracy: 0.4400: 100%|██████████| 25/25 [00:00<00:00, 36.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_acc_0.4889_train_loss_1.8364_val_acc_0.4316_val_loss_2.1447 epoch time 13.0970 seconds\n",
      "Generating Gradient Flow Plot at epoch 12\n",
      "Saving Gradient Flow Plot at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.6747, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 39.05it/s]\n",
      "loss: 2.3562, accuracy: 0.3800: 100%|██████████| 25/25 [00:00<00:00, 39.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_acc_0.4923_train_loss_1.8232_val_acc_0.4372_val_loss_2.1174 epoch time 12.8080 seconds\n",
      "Generating Gradient Flow Plot at epoch 13\n",
      "Saving Gradient Flow Plot at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.5853, accuracy: 0.5400: 100%|██████████| 475/475 [00:12<00:00, 39.11it/s]\n",
      "loss: 2.1829, accuracy: 0.4800: 100%|██████████| 25/25 [00:00<00:00, 40.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_acc_0.4938_train_loss_1.8125_val_acc_0.4496_val_loss_2.1221 epoch time 12.7689 seconds\n",
      "Generating Gradient Flow Plot at epoch 14\n",
      "Saving Gradient Flow Plot at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8584, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 38.40it/s]\n",
      "loss: 1.8667, accuracy: 0.5700: 100%|██████████| 25/25 [00:00<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_acc_0.4974_train_loss_1.8014_val_acc_0.4396_val_loss_2.0905 epoch time 13.2290 seconds\n",
      "Generating Gradient Flow Plot at epoch 15\n",
      "Saving Gradient Flow Plot at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8131, accuracy: 0.5000: 100%|██████████| 475/475 [00:12<00:00, 38.83it/s]\n",
      "loss: 1.8673, accuracy: 0.5100: 100%|██████████| 25/25 [00:00<00:00, 42.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_acc_0.5021_train_loss_1.7800_val_acc_0.4332_val_loss_2.1319 epoch time 12.8313 seconds\n",
      "Generating Gradient Flow Plot at epoch 16\n",
      "Saving Gradient Flow Plot at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8068, accuracy: 0.5400: 100%|██████████| 475/475 [00:12<00:00, 38.24it/s]\n",
      "loss: 2.0291, accuracy: 0.4600: 100%|██████████| 25/25 [00:00<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_acc_0.5034_train_loss_1.7764_val_acc_0.4424_val_loss_2.1168 epoch time 13.0722 seconds\n",
      "Generating Gradient Flow Plot at epoch 17\n",
      "Saving Gradient Flow Plot at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.0162, accuracy: 0.4500: 100%|██████████| 475/475 [00:12<00:00, 38.68it/s]\n",
      "loss: 1.9147, accuracy: 0.4500: 100%|██████████| 25/25 [00:00<00:00, 41.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: train_acc_0.5313_train_loss_1.6597_val_acc_0.4528_val_loss_2.0654 epoch time 12.8938 seconds\n",
      "Generating Gradient Flow Plot at epoch 26\n",
      "Saving Gradient Flow Plot at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.6963, accuracy: 0.5500: 100%|██████████| 475/475 [00:12<00:00, 39.44it/s]\n",
      "loss: 2.1898, accuracy: 0.4100: 100%|██████████| 25/25 [00:00<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: train_acc_0.5321_train_loss_1.6587_val_acc_0.4652_val_loss_2.0364 epoch time 12.6521 seconds\n",
      "Generating Gradient Flow Plot at epoch 27\n",
      "Saving Gradient Flow Plot at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.5947, accuracy: 0.4800: 100%|██████████| 475/475 [00:12<00:00, 39.22it/s]\n",
      "loss: 1.9604, accuracy: 0.4100: 100%|██████████| 25/25 [00:00<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: train_acc_0.5364_train_loss_1.6433_val_acc_0.4612_val_loss_2.0201 epoch time 12.7191 seconds\n",
      "Generating Gradient Flow Plot at epoch 28\n",
      "Saving Gradient Flow Plot at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3950, accuracy: 0.6300: 100%|██████████| 475/475 [00:12<00:00, 39.44it/s]\n",
      "loss: 1.7668, accuracy: 0.5300: 100%|██████████| 25/25 [00:00<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: train_acc_0.5423_train_loss_1.6201_val_acc_0.4732_val_loss_1.9521 epoch time 12.6833 seconds\n",
      "Generating Gradient Flow Plot at epoch 29\n",
      "Saving Gradient Flow Plot at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7638, accuracy: 0.5400: 100%|██████████| 475/475 [00:11<00:00, 39.59it/s]\n",
      "loss: 2.1249, accuracy: 0.4600: 100%|██████████| 25/25 [00:00<00:00, 42.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: train_acc_0.5439_train_loss_1.6168_val_acc_0.4636_val_loss_1.9859 epoch time 12.7846 seconds\n",
      "Generating Gradient Flow Plot at epoch 30\n",
      "Saving Gradient Flow Plot at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7762, accuracy: 0.4900:  55%|█████▌    | 263/475 [00:06<00:05, 41.40it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "loss: 1.3687, accuracy: 0.6600: 100%|██████████| 475/475 [00:11<00:00, 40.08it/s]\n",
      "loss: 2.0844, accuracy: 0.4600: 100%|██████████| 25/25 [00:00<00:00, 39.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: train_acc_0.5705_train_loss_1.5017_val_acc_0.4848_val_loss_1.9695 epoch time 12.4875 seconds\n",
      "Generating Gradient Flow Plot at epoch 40\n",
      "Saving Gradient Flow Plot at epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4546, accuracy: 0.6300: 100%|██████████| 475/475 [00:12<00:00, 38.63it/s]\n",
      "loss: 1.6200, accuracy: 0.5200: 100%|██████████| 25/25 [00:00<00:00, 38.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: train_acc_0.5844_train_loss_1.4423_val_acc_0.4928_val_loss_1.9243 epoch time 12.9483 seconds\n",
      "Generating Gradient Flow Plot at epoch 46\n",
      "Saving Gradient Flow Plot at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.2137, accuracy: 0.6200: 100%|██████████| 475/475 [00:12<00:00, 39.18it/s]\n",
      "loss: 2.1723, accuracy: 0.4100: 100%|██████████| 25/25 [00:00<00:00, 41.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: train_acc_0.5909_train_loss_1.4241_val_acc_0.4948_val_loss_1.9391 epoch time 12.7387 seconds\n",
      "Generating Gradient Flow Plot at epoch 47\n",
      "Saving Gradient Flow Plot at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3319, accuracy: 0.6300: 100%|██████████| 475/475 [00:11<00:00, 39.93it/s]\n",
      "loss: 2.0149, accuracy: 0.4800: 100%|██████████| 25/25 [00:00<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: train_acc_0.5950_train_loss_1.4096_val_acc_0.4732_val_loss_2.0560 epoch time 12.5347 seconds\n",
      "Generating Gradient Flow Plot at epoch 48\n",
      "Saving Gradient Flow Plot at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3329, accuracy: 0.6000: 100%|██████████| 475/475 [00:12<00:00, 38.08it/s]\n",
      "loss: 1.8737, accuracy: 0.5200: 100%|██████████| 25/25 [00:00<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: train_acc_0.5911_train_loss_1.4066_val_acc_0.4960_val_loss_1.9101 epoch time 13.1158 seconds\n",
      "Generating Gradient Flow Plot at epoch 49\n",
      "Saving Gradient Flow Plot at epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3267, accuracy: 0.6200: 100%|██████████| 475/475 [00:12<00:00, 38.82it/s]\n",
      "loss: 1.7723, accuracy: 0.4600: 100%|██████████| 25/25 [00:00<00:00, 40.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: train_acc_0.5970_train_loss_1.3957_val_acc_0.4936_val_loss_1.9160 epoch time 12.8637 seconds\n",
      "Generating Gradient Flow Plot at epoch 50\n",
      "Saving Gradient Flow Plot at epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.5318, accuracy: 0.5600:   3%|▎         | 13/475 [00:00<00:17, 26.89it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "loss: 1.1003, accuracy: 0.6500: 100%|██████████| 475/475 [00:12<00:00, 37.77it/s]\n",
      "loss: 1.7466, accuracy: 0.5300: 100%|██████████| 25/25 [00:00<00:00, 41.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: train_acc_0.6465_train_loss_1.2007_val_acc_0.5168_val_loss_1.8792 epoch time 13.1891 seconds\n",
      "Generating Gradient Flow Plot at epoch 72\n",
      "Saving Gradient Flow Plot at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1311, accuracy: 0.7100: 100%|██████████| 475/475 [00:11<00:00, 39.86it/s]\n",
      "loss: 1.6857, accuracy: 0.5500: 100%|██████████| 25/25 [00:00<00:00, 41.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: train_acc_0.6487_train_loss_1.1867_val_acc_0.5196_val_loss_1.8837 epoch time 12.5325 seconds\n",
      "Generating Gradient Flow Plot at epoch 73\n",
      "Saving Gradient Flow Plot at epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1677, accuracy: 0.7100: 100%|██████████| 475/475 [00:12<00:00, 38.78it/s]\n",
      "loss: 2.2852, accuracy: 0.4800: 100%|██████████| 25/25 [00:00<00:00, 39.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: train_acc_0.6506_train_loss_1.1852_val_acc_0.5148_val_loss_1.8708 epoch time 12.8905 seconds\n",
      "Generating Gradient Flow Plot at epoch 74\n",
      "Saving Gradient Flow Plot at epoch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1205, accuracy: 0.6700: 100%|██████████| 475/475 [00:12<00:00, 36.94it/s]\n",
      "loss: 2.2906, accuracy: 0.4400: 100%|██████████| 25/25 [00:00<00:00, 40.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: train_acc_0.6515_train_loss_1.1759_val_acc_0.5220_val_loss_1.8810 epoch time 13.6871 seconds\n",
      "Generating Gradient Flow Plot at epoch 75\n",
      "Saving Gradient Flow Plot at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3971, accuracy: 0.5900: 100%|██████████| 475/475 [00:12<00:00, 38.74it/s]\n",
      "loss: 1.8488, accuracy: 0.5200: 100%|██████████| 25/25 [00:00<00:00, 38.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: train_acc_0.6543_train_loss_1.1713_val_acc_0.5184_val_loss_1.8892 epoch time 12.9250 seconds\n",
      "Generating Gradient Flow Plot at epoch 76\n",
      "Saving Gradient Flow Plot at epoch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0195, accuracy: 0.7000:  16%|█▌        | 77/475 [00:02<00:09, 40.71it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "loss: 1.0486, accuracy: 0.7100: 100%|██████████| 475/475 [00:12<00:00, 39.17it/s]\n",
      "loss: 1.6025, accuracy: 0.5700: 100%|██████████| 25/25 [00:00<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: train_acc_0.6727_train_loss_1.0994_val_acc_0.5304_val_loss_1.8792 epoch time 12.9451 seconds\n",
      "Generating Gradient Flow Plot at epoch 96\n",
      "Saving Gradient Flow Plot at epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4417, accuracy: 0.5600: 100%|██████████| 475/475 [00:12<00:00, 38.97it/s]\n",
      "loss: 2.0026, accuracy: 0.5400: 100%|██████████| 25/25 [00:00<00:00, 40.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: train_acc_0.6709_train_loss_1.1033_val_acc_0.5256_val_loss_1.8835 epoch time 12.8173 seconds\n",
      "Generating Gradient Flow Plot at epoch 97\n",
      "Saving Gradient Flow Plot at epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1768, accuracy: 0.6600: 100%|██████████| 475/475 [00:12<00:00, 38.79it/s]\n",
      "loss: 1.7034, accuracy: 0.6000: 100%|██████████| 25/25 [00:00<00:00, 40.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: train_acc_0.6709_train_loss_1.1029_val_acc_0.5296_val_loss_1.8824 epoch time 12.8757 seconds\n",
      "Generating Gradient Flow Plot at epoch 98\n",
      "Saving Gradient Flow Plot at epoch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0955, accuracy: 0.7300: 100%|██████████| 475/475 [00:12<00:00, 38.70it/s]\n",
      "loss: 1.5020, accuracy: 0.5500: 100%|██████████| 25/25 [00:00<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: train_acc_0.6709_train_loss_1.0983_val_acc_0.5264_val_loss_1.8871 epoch time 12.8821 seconds\n",
      "Generating Gradient Flow Plot at epoch 99\n",
      "Saving Gradient Flow Plot at epoch 99\n",
      "Generating test set evaluation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8368, accuracy: 0.4800: 100%|██████████| 100/100 [00:01<00:00, 73.88it/s] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, test_loss, gradient_plt, all_gradients = conv_experiment.run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gradients[0].keys()\n",
    "\n",
    "layers = all_gradients[0]['layers']\n",
    "\n",
    "output_dict = {}\n",
    "output_dict['layers'] = layers\n",
    "for grad in  all_gradients:\n",
    "    gradient_deserialzied = [float(grad) for grad in grad['gradients']]\n",
    "    output_dict[f'epoch_{grad[\"epoch\"]}'] = gradient_deserialzied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "with open('vgg_38_batch_norm.json', 'w') as fp:\n",
    "    json.dump(output_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
